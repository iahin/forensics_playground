{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6adde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "import os\n",
    "\n",
    "def train_cifar(config, checkpoint_dir=None):\n",
    "    #net = net(config[\"l1\"], config[\"l2\"])\n",
    "    net = SiameseNetwork_GCN(\n",
    "        indim=config[\"l1\"], hiddendim=config[\"l2\"], outdim=config['outdim'], dropoutx=config['dropoutx'])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint\n",
    "    # should be restored.\n",
    "    if checkpoint_dir:\n",
    "        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state, optimizer_state = torch.load(checkpoint)\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "    \n",
    "    \n",
    "    validation_samples, traindata_list = train_sampler(X_train, y_train, dim=config[\"l1\"])\n",
    "    trainloader = DataLoader(traindata_list, batch_size=1, follow_batch=[\n",
    "                      'x1', 'x2'], shuffle=False)\n",
    "    \n",
    "    testdata_list = test_sampler(X_test, y_test, validation_samples, dim=config[\"l1\"])\n",
    "    valloader = DataLoader(testdata_list, batch_size=1,\n",
    "                         follow_batch=['x1', 'x2'], shuffle=False)\n",
    "\n",
    "#     trainloader = train_looood\n",
    "#     valloader = test_looood\n",
    "\n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            x1, x2 = data.x1.to(device), data.x2.to(device)\n",
    "            x1_index, x2_index = data.x1_index.to(\n",
    "                device), data.x2_index.to(device)\n",
    "            x1_batch, x2_batch = data.x1_batch.to(\n",
    "                device), data.x2_batch.to(device)\n",
    "            y = data.y.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            output1, output2 = net(x1, x1_index, x1_batch, x2, x2_index, x2_batch)\n",
    "            loss = criterion(output1, output2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            with torch.no_grad():\n",
    "                x1, x2 = data.x1.to(device), data.x2.to(device)\n",
    "                x1_index, x2_index = data.x1_index.to(device), data.x2_index.to(device)\n",
    "                x1_batch, x2_batch = data.x1_batch.to(\n",
    "                    device), data.x2_batch.to(device)\n",
    "                sim, _1, _2 = data.y.cpu().detach().numpy()\n",
    "\n",
    "                output1, output2 = net(x1, x1_index, x1_batch, x2, x2_index, x2_batch)\n",
    "                euclidean_distance = F.pairwise_distance(output1, output2).item()\n",
    "\n",
    "                predicted = None\n",
    "                if euclidean_distance < 1:\n",
    "                    predicted = 0\n",
    "                else:\n",
    "                    predicted = 1\n",
    "                    \n",
    "                total += 1\n",
    "                correct += (torch.tensor(predicted) == torch.tensor(sim)).sum().item()\n",
    "\n",
    "                loss = criterion(output1, output2, sim)\n",
    "                val_loss += loss.cpu().numpy()\n",
    "                val_steps += 1\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and will potentially be passed as the `checkpoint_dir`\n",
    "        # parameter in future iterations.\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save(\n",
    "                (net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
    "    print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples, max_num_epochs, gpus_per_trial):\n",
    "    config = {\n",
    "            \"l1\": 16,#tune.sample_from(lambda _: 2 ** np.random.randint(2, 6)),\n",
    "            \"l2\": 256,#tune.sample_from(lambda _: 2 ** np.random.randint(6, 9)),\n",
    "            \"outdim\": 3,#tune.sample_from(lambda _: np.random.randint(1, 10)),\n",
    "            \"lr\": tune.choice([0.0001]),\n",
    "            \"dropoutx\": tune.choice([0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "            \"batch_size\": tune.choice([1]),\n",
    "            #\"epoch\": tune.choice([20,30,40,50])\n",
    "        }\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    \n",
    "    result = tune.run(\n",
    "        tune.with_parameters(train_cifar),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 2, \n",
    "            \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "#     if ray.util.client.ray.is_connected():\n",
    "#         # If using Ray Client, we want to make sure checkpoint access\n",
    "#         # happens on the server. So we wrap `test_best_model` in a Ray task.\n",
    "#         # We have to make sure it gets executed on the same node that\n",
    "#         # ``tune.run`` is called on.\n",
    "#         from ray.tune.utils.util import force_on_current_node\n",
    "#         remote_fn = force_on_current_node(ray.remote(test_best_model))\n",
    "#         ray.get(remote_fn.remote(best_trial))\n",
    "    # else:\n",
    "    #     test_best_model(best_trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(num_samples=10, max_num_epochs=100, gpus_per_trial=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
